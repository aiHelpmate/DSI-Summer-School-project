{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Tumour Segmentation Network\n",
    "## Import packages\n",
    "Please make sure you have all the required packages installed for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Tumour Segmentation Network\n",
    "## Import packages\n",
    "Please make sure you have all the required packages installed for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brain Tumour Segmentation Network\n",
    "## Import packages\n",
    "Please make sure you have all the required packages installed for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise MRI Volume Slices and Segmentation Maps\n",
    "Each MRI image contains information about a three-dimensional (3D) volume of space. An MRI image is composed of a number of voxels, which are like pixels in 2D images. Here, try to visualise the axial plane (which usually has higher resolution) of some of the volumes and the corresponding segmentation maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e0be079",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ⚙️ Step: Skull Stripping (Simplified Example)\n",
    "# This is a dummy skull stripping using simple thresholding.\n",
    "# For real use, consider using brain extraction tools like FSL's BET.\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def skull_strip(image):\n",
    "    # Simple threshold to isolate brain region\n",
    "    _, mask = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY)\n",
    "    result = cv2.bitwise_and(image, image, mask=mask.astype(np.uint8))\n",
    "    return result\n",
    "\n",
    "# Example:\n",
    "# img = cv2.imread(\"sample_image.png\", 0)\n",
    "# stripped = skull_strip(img)\n",
    "# plt.imshow(stripped, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b124f21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ⚙️ Step: Image Registration using SimpleITK (rigid registration example)\n",
    "\n",
    "import SimpleITK as sitk\n",
    "\n",
    "def register_images(fixed_image_np, moving_image_np):\n",
    "    # Convert NumPy to SimpleITK images\n",
    "    fixed = sitk.GetImageFromArray(fixed_image_np.astype(np.float32))\n",
    "    moving = sitk.GetImageFromArray(moving_image_np.astype(np.float32))\n",
    "\n",
    "    # Registration setup\n",
    "    registration_method = sitk.ImageRegistrationMethod()\n",
    "    registration_method.SetMetricAsMeanSquares()\n",
    "    registration_method.SetOptimizerAsRegularStepGradientDescent(1.0, 1e-6, 200)\n",
    "    registration_method.SetInitialTransform(sitk.TranslationTransform(fixed.GetDimension()))\n",
    "    registration_method.SetInterpolator(sitk.sitkLinear)\n",
    "\n",
    "    # Execute registration\n",
    "    transform = registration_method.Execute(fixed, moving)\n",
    "    resampled = sitk.Resample(moving, fixed, transform, sitk.sitkLinear, 0.0, moving.GetPixelID())\n",
    "\n",
    "    return sitk.GetArrayFromImage(resampled)\n",
    "\n",
    "# Example usage:\n",
    "# fixed = cv2.imread(\"fixed.png\", 0)\n",
    "# moving = cv2.imread(\"moving.png\", 0)\n",
    "# registered = register_images(fixed, moving)\n",
    "# plt.imshow(registered, cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13fffb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ⚙️ Step: Resampling to uniform voxel spacing using SimpleITK\n",
    "\n",
    "def resample_image(image_np, new_spacing=(1.0, 1.0)):\n",
    "    image = sitk.GetImageFromArray(image_np.astype(np.float32))\n",
    "    original_spacing = image.GetSpacing()\n",
    "    original_size = image.GetSize()\n",
    "\n",
    "    # Compute new size\n",
    "    new_size = [\n",
    "        int(round(osz * ospc / nspc))\n",
    "        for osz, ospc, nspc in zip(original_size, original_spacing, new_spacing)\n",
    "    ]\n",
    "\n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    resample.SetOutputSpacing(new_spacing)\n",
    "    resample.SetSize(new_size)\n",
    "    resample.SetInterpolator(sitk.sitkLinear)\n",
    "    resample.SetOutputOrigin(image.GetOrigin())\n",
    "    resample.SetOutputDirection(image.GetDirection())\n",
    "\n",
    "    resampled = resample.Execute(image)\n",
    "    return sitk.GetArrayFromImage(resampled)\n",
    "\n",
    "# Example:\n",
    "# img = cv2.imread(\"input.png\", 0)\n",
    "# resampled_img = resample_image(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a single MRI slice and corresponding segmentation map\n",
    "def show_mri_slice(mri_path, mask_path=None, slice_index=50):\n",
    "    mri = nib.load(mri_path).get_fdata()\n",
    "    slice_img = mri[:, :, slice_index]\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1, 2 if mask_path else 1, 1)\n",
    "    plt.imshow(slice_img.T, cmap='gray', origin='lower')\n",
    "    plt.title('MRI Slice')\n",
    "    \n",
    "    if mask_path:\n",
    "        mask = nib.load(mask_path).get_fdata()\n",
    "        slice_mask = mask[:, :, slice_index]\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(slice_img.T, cmap='gray', origin='lower')\n",
    "        plt.imshow(slice_mask.T, cmap='Reds', alpha=0.5, origin='lower')\n",
    "        plt.title('With Mask Overlay')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing (Optional)\n",
    "\n",
    "Images in the original dataset are usually of different sizes, so sometimes we need to resize and normalise them (z-score is commonly used in preprocessing MRI images) to fit the CNN model. Depending on the images you choose to use for training your model, you may need to apply other preprocessing methods. If preprocessing methods like cropping are applied, remember to convert the segmentation result back to its original size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: preprocessing function (not required if using Dataset)\n",
    "def normalize_slice(slice):\n",
    "    return (slice - np.mean(slice)) / np.std(slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-time data augmentation\n",
    "Generalizability is crucial to a deep learning model, and it refers to the performance difference of a model when evaluated on seen data (training data) versus unseen data (testing data). Improving the generalizability of these models has always been a difficult challenge. \n",
    "\n",
    "**Data Augmentation** is an effective way of improving generalizability, because the augmented data will represent a more comprehensive set of possible data samples and minimize the distance between the training and validation/testing sets.\n",
    "\n",
    "There are many data augmentation methods you can choose from in this project, including rotation, shifting, flipping, etc. PyTorch provides excellent data augmentation capabilities through torchvision.transforms, which you can combine with custom transforms for medical imaging.\n",
    "\n",
    "You are encouraged to try different augmentation methods to get the best segmentation result.\n",
    "\n",
    "\n",
    "## Get the data generator ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define any transforms (optional)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a metric for the performance of the model\n",
    "The Dice score is used here to evaluate the performance of your model.\n",
    "More details about the Dice score and other metrics can be found at \n",
    "https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2. The Dice score can also be used as the loss function for training your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice coefficient metric\n",
    "def dice_coef(pred, target, epsilon=1e-6):\n",
    "    pred = (pred > 0.5).float()\n",
    "    target = target.float()\n",
    "    intersection = (pred * target).sum()\n",
    "    return (2. * intersection) / (pred.sum() + target.sum() + epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your own model here\n",
    "The U-Net (https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28) structure is widely used for medical image segmentation tasks. You can build your own model or modify the U-Net by changing the hyperparameters for our task. If you choose to use PyTorch, more information about PyTorch layers, including Conv2d, MaxPool2d, and Dropout, can be found at https://pytorch.org/docs/stable/nn.html. You can also explore popular PyTorch implementations of U-Net for medical image segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic U-Net model for 2D segmentation\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        def CBR(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        self.enc1 = CBR(1, 64)\n",
    "        self.enc2 = CBR(64, 128)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.dec1 = CBR(128, 64)\n",
    "        self.final = nn.Conv2d(64, 1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        d1 = self.dec1(self.up(e2))\n",
    "        out = self.final(d1)\n",
    "        return torch.sigmoid(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model here\n",
    "Once you have defined the model and data generator, you can start training your model. In PyTorch, you'll need to set up the training loop with the optimizer, loss function, and implement forward and backward passes manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No valid cases found in dataset_segmentation/train",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m image, mask\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Create dataset and dataloader\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m dataset \u001b[38;5;241m=\u001b[39m BrainMRISegmentationDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_segmentation/train\u001b[39m\u001b[38;5;124m'\u001b[39m, slice_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     43\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 19\u001b[0m, in \u001b[0;36mBrainMRISegmentationDataset.__init__\u001b[1;34m(self, root_dir, slice_index, transform)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\u001b[38;5;241m.\u001b[39mappend((flair_path, seg_path))\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo valid cases found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No valid cases found in dataset_segmentation/train"
     ]
    }
   ],
   "source": [
    "# Dataset class using real files\n",
    "class BrainMRISegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, slice_index=50, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.slice_index = slice_index\n",
    "        self.samples = []\n",
    "\n",
    "        for case in os.listdir(root_dir):\n",
    "            case_path = os.path.join(root_dir, case)\n",
    "            if not os.path.isdir(case_path):\n",
    "                continue\n",
    "            flair_path = os.path.join(case_path, f\"{case}_fla.nii\")\n",
    "            seg_path = os.path.join(case_path, f\"{case}_seg.nii\")\n",
    "            if os.path.exists(flair_path) and os.path.exists(seg_path):\n",
    "                self.samples.append((flair_path, seg_path))\n",
    "\n",
    "        if len(self.samples) == 0:\n",
    "            raise RuntimeError(f\"No valid cases found in {root_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        flair_path, seg_path = self.samples[idx]\n",
    "        flair_volume = nib.load(flair_path).get_fdata()\n",
    "        seg_volume = nib.load(seg_path).get_fdata()\n",
    "\n",
    "        flair_slice = flair_volume[:, :, self.slice_index]\n",
    "        seg_slice = seg_volume[:, :, self.slice_index]\n",
    "        flair_slice = (flair_slice - np.mean(flair_slice)) / np.std(flair_slice)\n",
    "\n",
    "        image = torch.tensor(flair_slice).unsqueeze(0).float()\n",
    "        mask = torch.tensor(seg_slice).unsqueeze(0).float()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = BrainMRISegmentationDataset('dataset_segmentation/train', slice_index=50, transform=None)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = UNet().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Train\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss, total_dice = 0, 0\n",
    "    for imgs, masks in tqdm(dataloader):\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(imgs)\n",
    "        loss = criterion(preds, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_dice += dice_coef(preds, masks).item()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss/len(dataloader):.4f} | Dice: {total_dice/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "Once your model is trained, remember to save it for testing. In PyTorch, you can save the model state dictionary using torch.save() for later loading and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"brain_tumour_segmentation.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model on the test set\n",
    "After your last Q&A session, you will be given the test set. Run your model on the test set to get the segmentation results and submit your results in a .zip file. If the MRI image is named '100_fla.nii.gz', save your segmentation result as '100_seg.nii.gz'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference and test loop placeholder\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for imgs, masks in dataloader:\n",
    "        imgs = imgs.to(device)\n",
    "        preds = model(imgs)\n",
    "        print(\"Pred shape:\", preds.shape)\n",
    "        break  # only run one batch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
